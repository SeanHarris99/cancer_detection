This directory contains code to train a convolutional neural net to detect cancerous lesions in gigapixel tissue images, provided by the CAMELYON16 challenge. Design was inspired by the paper [*Detecting Cancer Metasteses on Gigapixel Pathology Images*](https://arxiv.org/pdf/1703.02442.pdf) by Liu et al. 

The model trained on approximately 25% of the CAMELYON data, with a training AUC of 90%. Testing was done with visual inspection of heatmaps generated by the model on unseen biopsy slides, which are shown below.
 
 ![Alt text](figures/figure1.png?raw=true "Title")

The data set is large (500GB+) and unbalanced (50 to 1 majority class imbalance). Preprocessing was done to target cancerous areas and train on balanced data, inputs were randomly rotated and flipped to augment the minority class. For each input, a zoomed out view of the same region was simultaneously input, to give context to the target region.

 ![Alt text](figures/figure2.PNG?raw=true "Title2")
 
 Model architecture shown below. 
 
  ![Alt text](figures/figure3.PNG?raw=true "Title3")


Much more work is to be done. The model has not exhaustively trained on all the data, work is limited by storage space and download speed of the large dataset, along with I/O quotas on Google Drive, which is where data was kept. Little cross validation has been done on hyperparameters. Finally, a more robust validation metric needs to be designed, to test not just patch accuracy but success at finding all tumors in an image, each of which are composed of multiple patches. This has only been done visually thus far.

This project started as a final project in Joshua Gordon's 4995 Applied Deep Learning class at Columbia University in the Spring of 2021, and some of the above images were taken from a video made to explain the project code, found [here](https://www.youtube.com/watch?v=eHNgvjk_GxU). Note that the project has been significantly changed and improved since the end of said class.